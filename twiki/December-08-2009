---++!! OSG Council Monthly Teleconference

   * Teleconference Phone Number: 1-510-665-5437 
   * Meeting ID: 1111 
   * Date: Tuesday December 8th, 2009 
   * Time: 10:00 AM Pacific Time, 11AM Mountain, Noon Central, 1PM Eastern 
   * Attending: *Kent Blackburn, Paul Avery, Doug Olson, Shaowen, Wang, Tom Furlani, Michael Ernst, Chander Sehgal, Dan Fraser, Ruth Pordes, Brad Abbott, Richard Mount, Piotr Sliz, Horst Severini*
   * Regrets: *Bill Johnston, Scott McCauley* 

---++ *Agenda/Minutes*

---+++ Announcements

   * FNAL will host the next OSG All-Hands Meeting, March 8th - 10th 2009: [[http://indico.fnal.gov/conferenceDisplay.py?confId=2871][calendar]]
   * OSG Council Face-to-Face Meeting Thursday, March 11th 2009 at FNAL (day after All-Hands Meeting)
   * DOE Annual Report due January 1, 2010 (Need Science Stakeholder inputs)

---+++ Status Report from the OSG Executive Director, OSG Project Manager and/or Others

   * Ruth: [[https://twiki.grid.iu.edu/bin/view/Council/EDReport-Dec08-2009][OSG Executive Director Report]]
   * Chander: Project Management Report / San Diego TG Management &Allocations Meeting Summary (tentative)
   * Dan Fraser: [[https://twiki.grid.iu.edu/bin/view/Documentation/HighThroughputParallelComputing][OSG Production & HTPC]]

%BLUE% *minutes:*

Ruth encouraged people to read and send questions if there are any.

Chander went to the TG management and allocation meetings as the the OSG representative, consistent with the shared principles document. Purpose was two fold, one to understand how they do business, and two let them know that OSG's interest continues to be strong. Still working on list of milestones and schedule. Working to understand their allocation and accounting systems. Hope to get the appropriate accounting people from both TG and OSG together in the future as a goal.

Dan reported on the HTPC (High Throughput Parallel Computing) (see link on page below to wiki page). Did a press release in the last OSG Newsletter. This is something between the OSG model and the TG model, addressing a class of small to modestly parallel jobs that need to submit lots of jobs. Awarded recently with 3 PIs on the proposal (Miron, John McGee and Dan Fraser). Oklahoma first to open doors for these types of jobs. Currently have a series of chemistry users that have 8-way parallel jobs that run for about 24 hours each. The trick is to find 8-way boxes to ship the MPI libraries to allow all the MPI communications to run on a single node. This addresses the portability of the jobs. Ran over 26,000 wall hours at Oklahoma. Sometimes running as many as 50 jobs running concurrently there. Horst Severini reported it is going well, but that Gracia thinks that most jobs fail due to an exit code, but they are working on this. Also been working with Purdue, San Diego and Nebraska Firefly system. In the future they will look into 4 to 64 way jobs. Currently working with friendly sites, lots of other sites are out there. Also interested in trying out different batch systems to deal with allocating a single node as opposed to a core.

%ENDCOLOR%

---+++ Status Report from OSG Council Co-Chairs

   * Inputs to Annual DOE Written Report

%BLUE% *minutes:*

Paul has spoken to Alan Stone, our OSG DOE Program Officer about what he would like to see in the report. Alan has not yet looked in detail at the NSF report from this summer. We've gotten enough other feedback from DOE to indicate that we should provided more detail for HEP/NP in this report than was given in the NSF. CMS will probably have to make the biggest jump in the level of detail given in the report. More engagement details with OSG, technical issues, level of use, ... , more than general facts for the report. Paul should have forwarded to Michael for ATLAS and will do so. Historically we have turned in the report by December 31st. Paul talked to Alan Stone and he thinks as long as we get it in 3 months before the start of new funding (April 1st for 2010), that would be good enough, but Paul would like to push ahead with the schedule of having this in by Dec 31st. He has asked for inputs by Saturday of this week. Chander is collecting from the area coordinators. 

%ENDCOLOR%

---+++ Status Report for Pending Action Items (see [[https://twiki.grid.iu.edu/bin/view/Council/CouncilActionItems][List]])

   1. Promote the appointment of “Collective VO” representatives: *COMPLETED*
   1. Review current organizational structure for the OSG Council: *COMPLETED*
   1. Develop capability to measure the science domains being served by OSG resources: *COMPLETED - SEE ITEM 12*
   1. Evaluate concept of placing small scale parallel (MPI) clusters some OSG sites: *Owned by OSG Executive Team*
   1. Develop a plan to support virtual machine user/apps: *Owned by Jerome Lauret, Ruth Pordes, Sebastien Goasguen*
   1. Bring Area Coordinator Reporting to the monthly Council telecons: *Owned by Kent, Paul, Members of Executive Board*
   1. Evaluate how best to stagger Executive Director Term to account for current overlap with Council Co-Chairs and provide continuity: *Owned by Kent, Paul, Richard Mount*
   1. Evaluate how best to stagger Council Chair(s) Term(s) to account for current overlap with Executive Director and provide continuity: *Owned by Jerome Lauret, Piotr Sliz*
   1. Evaluate how best to establish and exercise Science Advisory Group (SAG): *Owned by Ruth Pordes, Scott McCauley?, Jerome Lauret, Piotr Sliz*
   1. Establish a subcommittee of the Open Science Grid (Executive Board) to determine how to migrate effort from core activities to provide support and architecture for the long term: *COUNCIL RECOMMENDATIONS DELIVERED TO OSG EB*
   1. Evaluate risks associated with the authentication and authorization systems: *Owned by Bill Johnston*
   1. Inform and iterate with the OSG VO Liaisons regarding the collection of science domain metrics. Work with VOs to accurately define their science domain areas for the purpose of these metrics: *Owned by Scott !McCauley*

%BLUE% *minutes:*

Action Item 4 was addressed by Dan Fraser's dicussion of HTPC. These are one in the same and Dan will keep the council updated on this. Previously know as the "adopt a cluster" proposal. This has been awarded and is being used (as FTE effort to work on this). Been working on this for about a month. 

Action Item 5 regarding virtual machines: Doug commented that there have been emails circulated regarding the DOE Cloud Computing effort (Magellan). STAR is already talking and SBGRID has expressed interest. Question: Is there someone in OSG that wants to coordinate this and express interest with the Magellan Project. Suggested that an email regarding this be sent to Jerome and Sebastien and cc the council to solicit interest.

Action Item 7 & 8: Piotr reported his thoughts on the evolution of the Council Chair seat. There is a write up from last Friday capturing this that has already been shared with Kent and Paul. Piotr will send it to Ruth today. Kent and Paul would like to discuss this outside today's call. Kent and Richard were concerned about the possibility of expanding the complexity of the role, but all points made by Piotr were certainly legitimate concerns. Kent and Paul may bring Richard in on the discussion of the write up. They will see if there is a way to structure this so that near term needs to address the continuation of funding can be folded into the longer term issues that came out of the development of the outline Piotr presented. Kent reported that there is a clear need for both the action item teams to work closely on this. His own views for the Executive Director were focused on the immediate needs (continuity and support for the new funding cycle) coming out of the in a way that allows for ultimately supporting the longer term visions.

Action Item 9: Ruth reported that arranging telephone calls with SAG is about a month behind. Information is flowing though.

Action Item 10: Miron owns this. Michael Ernst and Ian Fisk are providing effort from ATLAS and CMS to work on this. Two deliverables; one is the technology group, the other is the architectural document, which Ruth currently holds the baton for this. The architectural document is about a month late.

No other action item team members available to discuss other open items.

%ENDCOLOR%

---+++ Round-Table Virtual Organization Reports and Issues:

   * Report from Collective VO Representatives
   * Ongoing Goals/Milestones 
   * Planning for the Future

%BLUE% *minutes:*

Shaowen & Tom (Collective VO): Tom, Shaowen, Kent and Paul held a meeting earlier this week to organize ways to work with the collective VOs. One strategy is to hold a session at the All-Hands-Meeting to gather inputs from the VOs. Couple of things worth highlighting: group focusing on usability of the OSG, soliciting inputs from VOs; also effort to gather inputs from VOs for end-to-end information systems, along with workflow tools are used and what are the plans for VOs in the future. These will need some attention from Tom and Shaowen. NYState Grid is now part of this high performance portal in the State of New York.

Michael Ernst (ATLAS): Exciting times, single and collision beams, stable beam conditions until Christmas, beam lifetime on the order of 25 hours, expect to take one million collision events before Christmas, data replicated one hour after data taking, at T1s and after that at the T2s, analysis activities focusing on reconstruction, continue to see more and more people interested in the data, about 30 people looking at the data and growing, T3s are ramping up with collaboration coordinators and Condor Team coordinated by Dan Fraser to work on packaging issues to reduce effort needed to configure and deploy and has been valuable and successful at getting to a model that can be deployed by T3 administrators, working on architecture and one concern is the GT2 element, in Europe there is noticable effort looking into CREME developed by EGEE in Italy, BNL has a workshop going on now to look at this which included Condor Team, US ATLAS may in the future make a request to OSG to officially support the CREME package, all infrastructure components under the OSG umbrella have worked very well, issuing purchase orders for significant resource increases, T2s ramping up to 1Petabyte.

Brad Abbott (DZero): Everything is running very smoothly lately. Things are continuing to work more reliably and more efficiently. Sites have modified there policies which has improved the preemption issues seen earlier. Still an issue at MIT CMS that is being worked on. DZero would like to get a little more access to storage elements. They don't need that much, maybe 1TB or possibly as little as 0.5 TB. Monte Carlo guys are eager to try out a TeraGrid gateway once available.

%ENDCOLOR%

---+++ AOB

%BLUE% *minutes:*

None this month.

%ENDCOLOR%

-- Main.KentBlackburn - 03 Dec 2009