--+!! Executive Director’s report
%TOC% </td><td width="25%" align="right" valign="top">
 
---+ Executive Director’s report

Michael Ernst has accepted to be part of the Open Science Grid Executive Team as a replacement for Torre Wenaus as Torre takes up the position of US ATLAS S&C coordinator. Michael is already an active and productive partner!

Attended the Vision 2020 NEES and SDCI/STCI workshops. 

---++ LHC Operations:

The Operations Coordinator is now attending the WLCG operations meetings daily. The US ATLAS issue with the information at CERN was solved by a bug being found that resulted in stale entries. The GOC will implement ongoing monitoring. OSG management is taken action on these issues to increase the overall attention to and involvement in the daily operations priorities, decision making,  activities and follow through.

The transition to the  GGUS-GOC ticket exchange system  web services based system resulted in Alarm tickets for ATLAS being delayed. Also, requests from US ATLAS Tier-1 related to this issue were not responded to between Thursday and Monday. We take these failures very seriously and are looking into changes that would either add effort for monitoring and response to Alarm tickets or by-passing the critical-path nature of OSG in such cases.  Another issue is related to an operational issue discovered by ATLAS shifters at CERN on Sunday morning "ATLAS reports (Ueda) – […]BNL TAPE buffer became full (fixed, but no response in ggus:55326 -- GGUS-OSG issue"



---++ LIGO
LIGO hit a new record of 160K hours/day due to the help of many site administrators and Robert Engel, the OSG LIGO E@H staff.

---++ Campus Grids Meeting
The Campus Grid face-to-face meeting was regarded as very useful. However, because of the current diversity in implementations did not meet the original goals to “prepare a technical summary document describing and comparing current CG implementations, best practices and common patterns, as well as address technical issues moving forward..”
A report identifying questions to be answered and issues is available at https://twiki.grid.iu.edu/twiki/pub/Trash/Trash/Trash/Trash/CampusGrids/WorkingMeetingFermilab/cg-final.pdf
We plan to follow up on this report at the All Hands Meeting.

---++ OSG Futures core group
Useful discussions were held with US ATLAS, US CMS and OSG leaders to understand the response the experiments will make at the end of February to the request to describe their dependence on OSG. 


---++ Interactions with DOE - HEP and ASCR

At a further meeting with ASCR we presented the following for discussion about potential OSG contributions to leadership class computing needs:

Open Science Grid provides a general framework to which diverse resources can attach and benefit from. 

People come to us for 
   * For world class Distributed Computing Services and 
   * for leadership in distributed computing technologies, frameworks and methodologies

(OSG) Satellites concept provides framework for contributions to  and acceptance of externally developed technologies and services

All ideas will need a continuing full dialog between all parties – we appreciate any ASCR engagement in this. 

Capabilities that the HPC community is interested in that OSG might help with and has expertise in: 	
   * Managing the movement of data on and off the machines.
   * Benefit of “HTPC” – if there are users that need to run large ensembles of parallel jobs, then our current work  may be able to provide experience with techniques etc. 

Other more general areas of OSG work that might be of benefit:

   * Proven working collaboration for co-design, deployment and operational support across domain and computer scientists. Such teams take years in the making. 

   * System architecture Responsive to the needs of Collaborative Science/Research.
      * How to layer the software and capabilities in an end-to-end system.
      * How to discuss, decide and introduce new services and technologies.

Software
   * Trash/Trash/Integration of large number of software components to provide a uniform build, packaging and distribution service.
   * Build and test of tens of software components for multiple OSs/Platforms.
   * How to package, document and distribute an integrated set of  middleware for a diverse set of research communities.
   * Provide an anchor  project/community definitions, requirements, testing and acceptance of software developed by other programs  (extension of Satellite program) .  
   * Provision of configuration and valiation tools for installation of software on multiple platforms.

Job, data and storage management and scheduling methodologies and technologies.
	
#CouncilActionItem12
---++ Council Action Item 12
Inform and iterate with the OSG VO Liaisons regarding the collection of science domain metrics. Work with VOs to accurately define their science domain areas for the purpose of these metrics:

Report available now from MYOSG by VO: [[%ATTACHURL%/vosummary.pdf][VO to Science Mapping]] from OSG Registration Database


---++ Upcoming meetings:

[[https://twiki.grid.iu.edu/bin/view/Trash/DocumentationTeam/DocImprovementWorkshop][Documentation activity workshop: Feb 9/10]]<br> 
[[https://twiki.grid.iu.edu/bin/view/Trash/Trash/Trash/Trash/Blueprint/SharedStorage ][Trash/Trash/Blueprint on storage: Feb 11/12]]
<verbatim>
VOSS CMU project and OSG VOs: Feb 17/18
All Hands: Mar 8-11

</verbatim>




-- Main.RuthPordes - 06 Feb 2010



  
