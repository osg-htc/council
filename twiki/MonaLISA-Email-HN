---++ Harvey Newman's Email (March 13th, 2008 at 3:55PM Pacific)

Subject: Re: [Fwd: [Fwd: RE: OSG MonALISA Turn Down - GOC Ticket # 4740]]

Date: Thu, 13 Mar 2008 15:55:32 -0700

From: Harvey Newman <Harvey.Newman@cern.ch>

To: Rob Quick <rquick@iupui.edu>

CC: Ruth <ruth@fnal.gov>, Frank Wuerthwein <fkw@fnal.gov>, Paul Avery <avery@phys.ufl.edu>, 
Iosif Legrand <iosif.legrand@cern.ch>, "McKee, Shawn" <smckee@umich.edu>, Joel Butler <butler@fnal.gov>, 
Howard Gordon <gordon@bnl.gov>, Richard Cavanaugh <Richard.Cavanaugh@cern.ch>, 
"osg-eb@OPENSCIENCEGRID.ORG" <osg-eb@OPENSCIENCEGRID.ORG>

References: <47D84C95.5040007@cern.ch> <47D84E47.3090904@fnal.gov> <BE0E34EF-52E7-4520-B47B-0419C9B0ACE6@iupui.edu>

Hi,

Calculating the number of resources where it runs now
does not cover, and is largely unrelated to the main issue.

MonALISA based services will be used to allocate network
resources with any defined better-than best effort level of
service involving bandwidth between the US and CERN.

This is presented in some detail below. As this was foreshadowed
quite some time ago and now is enroute (we suggested starting
the integration also quite some time ago) it is no longer a point
of discussion.

Regards
Harvey


Rob Quick wrote:

> Ruth,
>
> Yes, I will send a stay of the MonALISA turn off date dependent on the 
> Council's decision.
>
> Let me touch on a few points, there are 81 OSG compute elements and 13 
> storage elements for a total of 94 registered resources on the OSG. 
> MonALISA is only running on CEs, so I won't include the SEs in the 
> next few calculations. 31 resources are reporting to MonALISA as of 
> last night, I do not recognize 4 of these resources and 5 reported no 
> informaiton. This leaves 22 of 81 resources or ~27%. This graph shows 
> DB size and a huge drop off in usage after the 0.6.0 release in March 
> of 2007. (http://www.grid.iu.edu/reports/ML_Jan07-Mar08.png)
>
> The effort involved includes maintenance, monitoring, and support of 
> MonALISA at the client and server level (aka Trouble Tickets reporting 
> problems, inaccuracies, outages, etc.). I agree that MonALISA has 
> evolved into be a stable service. But other than a few select 
> resources it is largely unused. I would suggest this is a VO level 
> service, and not an OSG Operations infrastructure service. If we would 
> like this to be an OSG service we need to think about making it a 
> mandatory service so we can use it fully. I am not disputing 
> MonALISA's usefulness as a monitoring tool, but until it is more 
> widely used, it is not helpful on a OSG-wide operations level.
>
> As I said, I will send a stay of the turn off date this morning 
> pending council decision.
>
> Rob
>
> On Mar 12, 2008, at 5:42 PM, Ruth wrote:
>
>> Hi Harvey, Rob Quick
>>
>> This needs to go through the OSG Council for the longer term decision 
>> as I have explained before. It is a matter of effort and priority of 
>> course. I am definitely and equially sure we don't need a crisis 
>> right now.
>>
>> However, with this email I am asking Rob Quick to retract from the 
>> recent decision to turn off the current ML service at the OSG 
>> operations center. I am, with this email, asking him to  let me know 
>> the effort it takes to maintain this service as others ramp up.
>>
>> Rob Quick - will you send the email retracting the decision or would 
>> you like me to?
>>
>> thank you for the email
>>
>> Ruth
>>
>>
>>
>> Harvey Newman wrote:
>>>
>>>
>>> Dear Ruth,
>>>
>>> I've been informed of this secondhand, but because of the urgency 
>>> and import,
>>> I am letting you know as OSG Executive Director that the MonALISA 
>>> turndown is,
>>> as explained in this note, is a strategic mistake of rare 
>>> proportions. Also the timing is quite
>>> out of synch. with the ramp up to LHC running, and the associated 
>>> network
>>> developments for managing network resources.
>>>
>>> As you can see ATLAS is responding to the turn-down immediately, in 
>>> recognition
>>> of the obvious fact that there is no monitoring system of similar 
>>> capability, or
>>> indeed in this class. As LHC ramps us, a system with this reach and 
>>> capability is
>>> required to keep track of what is going on. What it does provide 
>>> cannot be replaced
>>> by simpler, and in a fundamental way far more basic tools.
>>>
>>> Unlike many other manual systems, ML can deal with issues autonomously,
>>> or semi-autonomously, and problems do not generate reams of Emails from
>>> users and system managers where the failure is, and what its nature 
>>> is. As we've
>>> said many times, awareness of both end-systems and networks is a 
>>> requirement, and
>>> this requirement is increasing; not decreasing in any way.
>>>
>>> As you know, MonALISA underlies EVO, and you may also be aware
>>> that ALICE uses it to provide them with a highly capable autonomous
>>> infrastructure (and higher level services that they developed 
>>> themselves
>>> using its services) which supports computing operations
>>> with much reduced manpower.
>>>
>>> The major networks are transitioning to a new dynamic circuit-oriented
>>> paradigm to support the major network flows of the major science
>>> programs, most notably the LHC. In the case of the LHC, it is obvious
>>> that the LHC experiments will soon exhaust the affordable bandwidth
>>> and so managed services are being built, following a plan exposed to
>>> the community, including OSG, starting a few years ago. The 
>>> infrastucture
>>> to provide this has been installed, throughout the US and across the
>>> Atlantic by Internet2 and US LHCNet respectively, within the past year.
>>> A partnership of ESnet, GEANT2, the networks just mentioned,
>>> Fermilab, BNL and leading NRENs has begun work on a de facto
>>> standard way to build and provision inter-domain circuits.
>>>
>>> The services to be deployed on this infrastructure for LHC require 
>>> an end-to-end
>>> view and a true real-time monitoring. This is going to be based on 
>>> MonALISA
>>> as this is the one system of sufficient power and field proven 
>>> stability (over
>>> 7 years, around the clock) that exists. Applications, including OSG 
>>> middleware
>>> [sorry to repeat what I told the OSG management a few years ago] 
>>> will have
>>> to request network resources and communicate with network-resident 
>>> services
>>> that will manage and control the allocation of bandwidth. Projects 
>>> funded by
>>> NSF (UltraLight, PLaNetS) are working with US LHCNet on some of these
>>> services as well.
>>>
>>> The APIs are starting to be developed. The communication will be 
>>> real-time in that
>>> the allocated network resources will be checked as being well-used, 
>>> or not,
>>> and resources not well-used will be freed to satisfy competing 
>>> requests.
>>> The configuration of the end-systems in terms of hardware and 
>>> software as
>>> well as state (load, traffic levels) will be used to diagnose 
>>> throughput problems,
>>> together with realtime diagnosis of the network itself, to demystify 
>>> what is happening
>>> to flows that do not perform in a way that matches (to some degree 
>>> of accuracy)
>>> what was requested.
>>>
>>> So OSG, using the ML infrastructure, at least had a start in this 
>>> correct direction.
>>> If it ML monitoring is turned down, then either much greater effort 
>>> will have to be invested
>>> by OSG to restore (as well as extend and expand on, as above) this 
>>> capability, or OSG's
>>> ability to distribute data will be correspondingly reduced, by a 
>>> major factor,
>>> as the competition for network resources increases with the arrival 
>>> of real LHC
>>> data. The reason is simply that as a strategic, scarce resource, 
>>> Network resources
>>> with a guaranteed bandwidth will be assigned to requests where the 
>>> requesting application
>>> can communicate with the network services, and which are configured 
>>> to use the network
>>> resources allocated to them at reasonably high efficiency. Otherwise 
>>> requests will have
>>> go to a catch-all "best effort" class to which a relatively small 
>>> amount of bandwidth will
>>> be assigned.
>>>
>>> On a related topic, the above has been presented in some detail at DOE
>>> at the US LHCNet review in mid-January and today at NSF. In the
>>> discussion that ensued today, it was clear that OSG needs to 
>>> re-engage with
>>> these network issues, through the network working group that was formed
>>> a few years ago with the leadership of Don and Shawn, but was not 
>>> funded
>>> by OSG project management.
>>>
>>> Best regards
>>> Harvey
>>>
>>>
>>>
>>>
>>> Subject:
>>> [Fwd: RE: OSG MonALISA Turn Down - GOC Ticket # 4740]
>>> From:
>>> Iosif Legrand <Iosif.Legrand@cern.ch>
>>> Date:
>>> Wed, 12 Mar 2008 21:00:03 +0100
>>> To:
>>> Harvey Newman <Harvey.Newman@cern.ch>
>>> To:
>>> Harvey Newman <Harvey.Newman@cern.ch>
>>> CC:
>>> Iosif Legrand <Iosif.Legrand@cern.ch>
>>>
>>> Hello Harvey,
>>>
>>>
>>> I received the attached email from the USATLAS , and they want a ML
>>> repository to replace the OSG site .
>>> I think it will be useful for us to support this... and I hope to 
>>> provide a
>>> good example for the USCMS.
>>> I think  CMS did a big mistake with  the CERN dashboard.
>>> In fact the CMS dashboard is totally depended on ML for collecting all
>>> the jobs related information.  As I told you, it is done in a very 
>>> inefficient way and
>>> is  using now only one  ML service running  at CERN.
>>> This ML service collects ~ 200 000 parameters with an average update
>>> rate of ~ 1500 parameters per second from all the  CMS jobs running 
>>> on all CMS centers
>>> This ML service works really fine for more than two years, but this 
>>> is not an architecture to monitor the
>>> jobs in CMS.
>>>
>>>
>>>
>>> Best Regards,
>>> ioji
>>>
>>>
>>> -------- Original Message --------
>>> Subject:     RE: OSG MonALISA Turn Down - GOC Ticket # 4740
>>> Date:     Wed, 12 Mar 2008 13:53:14 -0500 (CDT)
>>> From:     Horst Severini <hs@nhn.ou.edu>
>>> Reply-To:     Horst Severini <hs@nhn.ou.edu>
>>> To:     rwg@hep.uchicago.edu, packardj@rcf.rhic.bnl.gov, 
>>> smckee@umich.edu
>>> CC:     hs@nhn.ou.edu, aglt2-hardware@umich.edu, mernst@bnl.gov, 
>>> Iosif.Legrand@cern.ch, Costin.Grigoras@cern.ch
>>>
>>>
>>>
>>> Hi Ioji and Costin,
>>>
>>> we (USATLAS) were talking about this OSG ML Repository shutdown this 
>>> morning,
>>> and we'd like to migrate/replicate the Repository to/at BNL, since 
>>> we really like all the useful features of the Repository and don't 
>>> want to lose it.
>>>
>>> How difficult would that be? Could you help us with the configuration?
>>>
>>> Thanks a lot,
>>>
>>>     Horst
>>>
>>>
>


-- Main.KentBlackburn - 01 Apr 2008